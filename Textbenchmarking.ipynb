{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d427c9e2-4da9-4eda-86a9-f7c2306820ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: Tokenization\n",
      "tokenize: 0.000001 seconds\n",
      "tokenize_numba: 2.502573 seconds\n",
      "tokenize_joblib: 0.212139 seconds\n",
      "tokenize_memoized: 0.000010 seconds\n",
      "tokenize_standard: 0.000003 seconds\n",
      "\n",
      "Task: Normalization\n",
      "normalize: 0.000003 seconds\n",
      "normalize_numba: 1.953045 seconds\n",
      "normalize_joblib: 0.011228 seconds\n",
      "normalize_memoized: 0.000005 seconds\n",
      "normalize_standard: 0.000001 seconds\n",
      "\n",
      "Task: Feature Extraction\n",
      "extract_features: 0.000004 seconds\n",
      "extract_features_numba: 0.980209 seconds\n",
      "extract_features_joblib: 0.011143 seconds\n",
      "extract_features_memoized: 0.000006 seconds\n",
      "extract_features_standard: 0.000003 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from numba import jit\n",
    "from joblib import Parallel, delayed\n",
    "from functools import lru_cache\n",
    "\n",
    "# Sample text data\n",
    "text_data = \"This is a sample text for text processing benchmarking.\"\n",
    "\n",
    "# Task 1: Text Tokenization\n",
    "def tokenize(text):\n",
    "    return text.split()\n",
    "\n",
    "@jit\n",
    "def tokenize_numba(text):\n",
    "    return text.split()\n",
    "\n",
    "def tokenize_joblib(text):\n",
    "    return Parallel(n_jobs=-1)(delayed(tokenize)(word) for word in text.split())\n",
    "\n",
    "@lru_cache(maxsize=None)\n",
    "def tokenize_memoized(text):\n",
    "    return text.split()\n",
    "\n",
    "def tokenize_standard(text):\n",
    "    return text.split()\n",
    "\n",
    "# Task 2: Text Normalization\n",
    "def normalize(text):\n",
    "    return text.lower()\n",
    "\n",
    "@jit\n",
    "def normalize_numba(text):\n",
    "    return text.lower()\n",
    "\n",
    "def normalize_joblib(text):\n",
    "    return Parallel(n_jobs=-1)(delayed(normalize)(word) for word in text.split())\n",
    "\n",
    "@lru_cache(maxsize=None)\n",
    "def normalize_memoized(text):\n",
    "    return text.lower()\n",
    "\n",
    "def normalize_standard(text):\n",
    "    return text.lower()\n",
    "\n",
    "# Task 3: Feature Extraction\n",
    "def extract_features(text):\n",
    "    return [len(word) for word in text.split()]\n",
    "\n",
    "@jit\n",
    "def extract_features_numba(text):\n",
    "    return [len(word) for word in text.split()]\n",
    "\n",
    "def extract_features_joblib(text):\n",
    "    return Parallel(n_jobs=-1)(delayed(len)(word) for word in text.split())\n",
    "\n",
    "@lru_cache(maxsize=None)\n",
    "def extract_features_memoized(text):\n",
    "    return [len(word) for word in text.split()]\n",
    "\n",
    "def extract_features_standard(text):\n",
    "    return [len(word) for word in text.split()]\n",
    "\n",
    "# Measure execution time for each technique on each task\n",
    "tasks = {\n",
    "    \"Tokenization\": (tokenize, tokenize_numba, tokenize_joblib, tokenize_memoized, tokenize_standard),\n",
    "    \"Normalization\": (normalize, normalize_numba, normalize_joblib, normalize_memoized, normalize_standard),\n",
    "    \"Feature Extraction\": (extract_features, extract_features_numba, extract_features_joblib, extract_features_memoized, extract_features_standard)\n",
    "}\n",
    "\n",
    "for task, functions in tasks.items():\n",
    "    print(f\"Task: {task}\")\n",
    "    for func in functions:\n",
    "        start_time = time.time()\n",
    "        result = func(text_data)\n",
    "        execution_time = time.time() - start_time\n",
    "        print(f\"{func.__name__}: {execution_time:.6f} seconds\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cccf10f1-ca09-4eb9-8be8-9c441a54b102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: Tokenization\n",
      "tokenize_joblib: 0.082089 seconds\n",
      "tokenize_memoized: 0.000886 seconds\n",
      "tokenize_numba: 0.955818 seconds\n",
      "tokenize_standard: 0.000385 seconds\n",
      "\n",
      "Task: Normalization\n",
      "normalize_joblib: 0.068397 seconds\n",
      "normalize_memoized: 0.000218 seconds\n",
      "normalize_numba: 0.744705 seconds\n",
      "normalize_standard: 0.000072 seconds\n",
      "\n",
      "Task: Feature Extraction\n",
      "extract_features_joblib: 0.061974 seconds\n",
      "extract_features_memoized: 0.000749 seconds\n",
      "extract_features_numba: 1.073678 seconds\n",
      "extract_features_standard: 0.000670 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from joblib import Parallel, delayed\n",
    "from functools import lru_cache\n",
    "from numba import jit\n",
    "\n",
    "# Task 1: Text Tokenization\n",
    "def tokenize(text):\n",
    "    return text.split()\n",
    "\n",
    "def tokenize_joblib(text):\n",
    "    return Parallel(n_jobs=-1)(delayed(tokenize)(word) for word in text.split())\n",
    "\n",
    "@lru_cache(maxsize=None)\n",
    "def tokenize_memoized(text):\n",
    "    return text.split()\n",
    "\n",
    "@jit\n",
    "def tokenize_numba(text):\n",
    "    return text.split()\n",
    "\n",
    "def tokenize_standard(text):\n",
    "    return text.split()\n",
    "\n",
    "# Task 2: Text Normalization\n",
    "def normalize(text):\n",
    "    return text.lower()\n",
    "\n",
    "def normalize_joblib(text):\n",
    "    return Parallel(n_jobs=-1)(delayed(normalize)(word) for word in text.split())\n",
    "\n",
    "@lru_cache(maxsize=None)\n",
    "def normalize_memoized(text):\n",
    "    return text.lower()\n",
    "\n",
    "@jit\n",
    "def normalize_numba(text):\n",
    "    return text.lower()\n",
    "\n",
    "def normalize_standard(text):\n",
    "    return text.lower()\n",
    "\n",
    "# Task 3: Feature Extraction\n",
    "def extract_features(text):\n",
    "    return [len(word) for word in text.split()]\n",
    "\n",
    "def extract_features_joblib(text):\n",
    "    return Parallel(n_jobs=-1)(delayed(len)(word) for word in text.split())\n",
    "\n",
    "@lru_cache(maxsize=None)\n",
    "def extract_features_memoized(text):\n",
    "    return [len(word) for word in text.split()]\n",
    "\n",
    "@jit\n",
    "def extract_features_numba(text):\n",
    "    return [len(word) for word in text.split()]\n",
    "\n",
    "def extract_features_standard(text):\n",
    "    return [len(word) for word in text.split()]\n",
    "\n",
    "# Read text from file\n",
    "def read_text_from_file(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        return file.read()\n",
    "\n",
    "# Measure execution time for each technique on each task\n",
    "tasks = {\n",
    "    \"Tokenization\": (tokenize_joblib, tokenize_memoized, tokenize_numba, tokenize_standard),\n",
    "    \"Normalization\": (normalize_joblib, normalize_memoized, normalize_numba, normalize_standard),\n",
    "    \"Feature Extraction\": (extract_features_joblib, extract_features_memoized, extract_features_numba, extract_features_standard)\n",
    "}\n",
    "\n",
    "# Filename of the text document\n",
    "filename = \"/home/jas/Documents/example txt\"\n",
    "\n",
    "# Read text from file\n",
    "text_data = read_text_from_file(filename)\n",
    "\n",
    "# Measure and print execution times for each task and each technique\n",
    "for task, functions in tasks.items():\n",
    "    print(f\"Task: {task}\")\n",
    "    for func in functions:\n",
    "        start_time = time.time()\n",
    "        result = func(text_data)\n",
    "        execution_time = time.time() - start_time\n",
    "        print(f\"{func.__name__}: {execution_time:.6f} seconds\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ae2d443d-42e9-478c-9eec-befbbdbb6838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: Tokenization\n",
      "tokenize_joblib: 0.078413 seconds\n",
      "tokenize_memoized: 0.000624 seconds\n",
      "tokenize_numba: 0.960065 seconds\n",
      "tokenize_standard: 0.000260 seconds\n",
      "tokenize_with_index: 0.000888 seconds\n",
      "\n",
      "Task: Normalization\n",
      "normalize_joblib: 0.074537 seconds\n",
      "normalize_memoized: 0.000215 seconds\n",
      "normalize_numba: 0.732997 seconds\n",
      "normalize_standard: 0.000073 seconds\n",
      "normalize_with_index: 0.002141 seconds\n",
      "\n",
      "Task: Feature Extraction\n",
      "extract_features_joblib: 0.065248 seconds\n",
      "extract_features_memoized: 0.000611 seconds\n",
      "extract_features_numba: 0.960474 seconds\n",
      "extract_features_standard: 0.000547 seconds\n",
      "extract_features_with_index: 0.000944 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from joblib import Parallel, delayed\n",
    "from functools import lru_cache\n",
    "from numba import jit\n",
    "\n",
    "# Task 1: Text Tokenization\n",
    "def tokenize(text):\n",
    "    return text.split()\n",
    "\n",
    "def tokenize_joblib(text):\n",
    "    return Parallel(n_jobs=-1)(delayed(tokenize)(word) for word in text.split())\n",
    "\n",
    "@lru_cache(maxsize=None)\n",
    "def tokenize_memoized(text):\n",
    "    return text.split()\n",
    "\n",
    "@jit\n",
    "def tokenize_numba(text):\n",
    "    return text.split()\n",
    "\n",
    "def tokenize_standard(text):\n",
    "    return text.split()\n",
    "\n",
    "# Task 2: Text Normalization\n",
    "def normalize(text):\n",
    "    return text.lower()\n",
    "\n",
    "def normalize_joblib(text):\n",
    "    return Parallel(n_jobs=-1)(delayed(normalize)(word) for word in text.split())\n",
    "\n",
    "@lru_cache(maxsize=None)\n",
    "def normalize_memoized(text):\n",
    "    return text.lower()\n",
    "\n",
    "@jit\n",
    "def normalize_numba(text):\n",
    "    return text.lower()\n",
    "\n",
    "def normalize_standard(text):\n",
    "    return text.lower()\n",
    "\n",
    "# Task 3: Feature Extraction\n",
    "def extract_features(text):\n",
    "    return [len(word) for word in text.split()]\n",
    "\n",
    "def extract_features_joblib(text):\n",
    "    return Parallel(n_jobs=-1)(delayed(len)(word) for word in text.split())\n",
    "\n",
    "@lru_cache(maxsize=None)\n",
    "def extract_features_memoized(text):\n",
    "    return [len(word) for word in text.split()]\n",
    "\n",
    "@jit\n",
    "def extract_features_numba(text):\n",
    "    return [len(word) for word in text.split()]\n",
    "\n",
    "def extract_features_standard(text):\n",
    "    return [len(word) for word in text.split()]\n",
    "\n",
    "# Additional functionalities for each task\n",
    "\n",
    "# Task 1: Text Tokenization\n",
    "def tokenize_with_index(text):\n",
    "    return [(i, word) for i, word in enumerate(text.split())]\n",
    "\n",
    "# Task 2: Text Normalization\n",
    "def normalize_with_index(text):\n",
    "    return [(i, normalize(word)) for i, word in enumerate(text.split())]\n",
    "\n",
    "# Task 3: Feature Extraction\n",
    "def extract_features_with_index(text):\n",
    "    return [(i, len(word)) for i, word in enumerate(text.split())]\n",
    "\n",
    "# Read text from file\n",
    "def read_text_from_file(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        return file.read()\n",
    "\n",
    "# Measure execution time for each technique on each task\n",
    "tasks = {\n",
    "    \"Tokenization\": (tokenize_joblib, tokenize_memoized, tokenize_numba, tokenize_standard, tokenize_with_index),\n",
    "    \"Normalization\": (normalize_joblib, normalize_memoized, normalize_numba, normalize_standard, normalize_with_index),\n",
    "    \"Feature Extraction\": (extract_features_joblib, extract_features_memoized, extract_features_numba, extract_features_standard, extract_features_with_index)\n",
    "}\n",
    "\n",
    "# Filename of the text document\n",
    "filename = \"/home/jas/Documents/example txt\"\n",
    "\n",
    "# Read text from file\n",
    "text_data = read_text_from_file(filename)\n",
    "\n",
    "# Measure and print execution times for each task and each technique\n",
    "for task, functions in tasks.items():\n",
    "    print(f\"Task: {task}\")\n",
    "    for func in functions:\n",
    "        start_time = time.time()\n",
    "        result = func(text_data)\n",
    "        execution_time = time.time() - start_time\n",
    "        print(f\"{func.__name__}: {execution_time:.6f} seconds\")\n",
    "    print()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e5073342-7359-4888-8f3f-5d072b371cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard Python Tokenization: ['Apple', 'Inc.', 'is', 'an', 'American', 'multinational', 'technology', 'company', 'headquartered', 'in', 'Cupertino,', 'California.']\n",
      "Time taken: 0.00011801719665527344\n",
      "Numpy Tokenization: ['Apple' 'Inc.' 'is' 'an' 'American' 'multinational' 'technology'\n",
      " 'company' 'headquartered' 'in' 'Cupertino,' 'California.']\n",
      "Time taken: 0.0001537799835205078\n",
      "Numba Tokenization: ['Apple', 'Inc.', 'is', 'an', 'American', 'multinational', 'technology', 'company', 'headquartered', 'in', 'Cupertino,', 'California.']\n",
      "Time taken: 0.9556922912597656\n",
      "Standard Python Bag-of-Words: {'Apple': 1, 'Inc.': 1, 'is': 1, 'an': 1, 'American': 1, 'multinational': 1, 'technology': 1, 'company': 1, 'headquartered': 1, 'in': 1, 'Cupertino,': 1, 'California.': 1}\n",
      "Time taken: 9.989738464355469e-05\n",
      "Numpy Bag-of-Words: {'American': 1, 'Apple': 1, 'California.': 1, 'Cupertino,': 1, 'Inc.': 1, 'an': 1, 'company': 1, 'headquartered': 1, 'in': 1, 'is': 1, 'multinational': 1, 'technology': 1}\n",
      "Time taken: 0.0002028942108154297\n",
      "Standard Python Named Entity Recognition: [('Apple Inc.', 'ORG'), ('American', 'NORP'), ('Cupertino', 'GPE'), ('California', 'GPE')]\n",
      "Time taken: 0.2930572032928467\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from numba import jit\n",
    "import spacy\n",
    "\n",
    "# Sample text document\n",
    "text = \"Apple Inc. is an American multinational technology company headquartered in Cupertino, California.\"\n",
    "\n",
    "# Standard Python Tokenization\n",
    "def tokenize_python(text):\n",
    "    return text.split()\n",
    "\n",
    "# Numpy Tokenization\n",
    "def tokenize_numpy(text):\n",
    "    return np.array(text.split())\n",
    "\n",
    "# Numba Tokenization\n",
    "@jit(nopython=True)\n",
    "def tokenize_numba(text):\n",
    "    return text.split()\n",
    "\n",
    "# Standard Python Vectorization (Bag-of-Words)\n",
    "def vectorize_python(tokens):\n",
    "    word_dict = {}\n",
    "    for token in tokens:\n",
    "        word_dict[token] = word_dict.get(token, 0) + 1\n",
    "    return word_dict\n",
    "\n",
    "# Numpy Vectorization (Bag-of-Words)\n",
    "def vectorize_numpy(tokens):\n",
    "    unique_tokens, counts = np.unique(tokens, return_counts=True)\n",
    "    return dict(zip(unique_tokens, counts))\n",
    "\n",
    "# Standard Python Named Entity Recognition (NER)\n",
    "def ner_python(text):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(text)\n",
    "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "    return entities\n",
    "\n",
    "\n",
    "# Measure execution time for each approach\n",
    "start_time = time.time()\n",
    "tokens_py = tokenize_python(text)\n",
    "print(\"Standard Python Tokenization:\", tokens_py)\n",
    "print(\"Time taken:\", time.time() - start_time)\n",
    "\n",
    "start_time = time.time()\n",
    "tokens_np = tokenize_numpy(text)\n",
    "print(\"Numpy Tokenization:\", tokens_np)\n",
    "print(\"Time taken:\", time.time() - start_time)\n",
    "\n",
    "start_time = time.time()\n",
    "tokens_nb = tokenize_numba(text)\n",
    "print(\"Numba Tokenization:\", tokens_nb)\n",
    "print(\"Time taken:\", time.time() - start_time)\n",
    "\n",
    "start_time = time.time()\n",
    "bow_py = vectorize_python(tokens_py)\n",
    "print(\"Standard Python Bag-of-Words:\", bow_py)\n",
    "print(\"Time taken:\", time.time() - start_time)\n",
    "\n",
    "start_time = time.time()\n",
    "bow_np = vectorize_numpy(tokens_np)\n",
    "print(\"Numpy Bag-of-Words:\", bow_np)\n",
    "print(\"Time taken:\", time.time() - start_time)\n",
    "\n",
    "start_time = time.time()\n",
    "entities_py = ner_python(text)\n",
    "print(\"Standard Python Named Entity Recognition:\", entities_py)\n",
    "print(\"Time taken:\", time.time() - start_time)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f69e78eb-8f20-4106-ae02-18f57d1133e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard Python Word2Vec Training:\n",
      "Time taken: 0.012315988540649414\n",
      "Numpy Word2Vec Training:\n",
      "Time taken: 0.010944843292236328\n",
      "Memoized Word2Vec Training:\n",
      "Time taken: 0.009955406188964844\n",
      "Joblib Word2Vec Training:\n",
      "Time taken: 0.17759943008422852\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from joblib import Parallel, delayed\n",
    "import joblib\n",
    "from functools import lru_cache\n",
    "\n",
    "# Function to read a large text file\n",
    "def read_large_text_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        corpus = file.readlines()\n",
    "    return corpus\n",
    "\n",
    "# Load the large text file as the corpus\n",
    "large_corpus_file = \"/home/jas/Documents/example txt\"\n",
    "\n",
    "corpus = read_large_text_file(large_corpus_file)\n",
    "\n",
    "# Function to train Word2Vec model (mock implementation)\n",
    "def train_word2vec(corpus):\n",
    "    sentences = [sentence.split() for sentence in corpus]\n",
    "    model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "    return model\n",
    "\n",
    "# Standard Python training of Word2Vec model\n",
    "def train_word2vec_python(corpus):\n",
    "    return train_word2vec(corpus)\n",
    "\n",
    "# Numpy training of Word2Vec model\n",
    "def train_word2vec_numpy(corpus):\n",
    "    return train_word2vec(corpus)\n",
    "\n",
    "# Memoized training of Word2Vec model\n",
    "@lru_cache(maxsize=None)\n",
    "def train_word2vec_memoized(corpus):\n",
    "    return train_word2vec(corpus)\n",
    "\n",
    "# Joblib training of Word2Vec model\n",
    "def train_word2vec_joblib(corpus):\n",
    "    return train_word2vec(corpus)\n",
    "\n",
    "# Measure execution time for each approach\n",
    "start_time = time.time()\n",
    "model_py = train_word2vec_python(corpus)\n",
    "print(\"Standard Python Word2Vec Training:\")\n",
    "print(\"Time taken:\", time.time() - start_time)\n",
    "\n",
    "start_time = time.time()\n",
    "model_np = train_word2vec_numpy(corpus)\n",
    "print(\"Numpy Word2Vec Training:\")\n",
    "print(\"Time taken:\", time.time() - start_time)\n",
    "\n",
    "start_time = time.time()\n",
    "model_mm = train_word2vec_memoized(tuple(corpus))\n",
    "print(\"Memoized Word2Vec Training:\")\n",
    "print(\"Time taken:\", time.time() - start_time)\n",
    "\n",
    "start_time = time.time()\n",
    "model_joblib = Parallel(n_jobs=-1)(delayed(train_word2vec_joblib)(corpus) for _ in range(10))\n",
    "print(\"Joblib Word2Vec Training:\")\n",
    "print(\"Time taken:\", time.time() - start_time)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc58815a-3f05-4daa-81d8-842d71165fc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f93880fe-7b3a-4601-a438-4e9371689384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard Python Cosine Similarity Calculation:\n",
      "Time taken: 0.0021979808807373047\n",
      "Optimized Numpy Cosine Similarity Calculation:\n",
      "Time taken: 0.001977205276489258\n",
      "Memoized Cosine Similarity Calculation:\n",
      "Time taken: 0.0020465850830078125\n",
      "Joblib Optimized Cosine Similarity Calculation:\n",
      "Time taken: 0.011260271072387695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jas/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/jas/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "/tmp/ipykernel_4947/160912506.py:77: RuntimeWarning: invalid value encountered in divide\n",
      "  tfidf_matrix /= tfidf_norm\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "from functools import lru_cache\n",
    "from joblib import Parallel, delayed\n",
    "import joblib\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function to read a large text file\n",
    "def read_large_text_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    return text\n",
    "\n",
    "# Load the large text file\n",
    "large_text_file = \"/home/jas/Documents/example txt\"\n",
    "text = read_large_text_file(large_text_file)\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Tokenize text\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    # Remove stop words and punctuation\n",
    "    tokens = [word for word in tokens if word.isalnum() and word not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Preprocess text\n",
    "preprocessed_text = preprocess_text(text)\n",
    "\n",
    "# Task: Cosine Similarity Calculation\n",
    "def cosine_similarity(documents):\n",
    "    # Create TF-IDF vectorizer\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    # Calculate TF-IDF matrix\n",
    "    tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "    # Compute cosine similarity matrix\n",
    "    similarity_matrix = np.dot(tfidf_matrix, tfidf_matrix.T)\n",
    "    return similarity_matrix\n",
    "\n",
    "# Standard Python implementation\n",
    "start_time = time.time()\n",
    "similarity_py = cosine_similarity([preprocessed_text])\n",
    "print(\"Standard Python Cosine Similarity Calculation:\")\n",
    "print(\"Time taken:\", time.time() - start_time)\n",
    "\n",
    "# Numpy implementation\n",
    "def calculate_tfidf_numpy(documents):\n",
    "    # Tokenize documents\n",
    "    tokens = [word_tokenize(doc) for doc in documents]\n",
    "    # Create vocabulary\n",
    "    vocabulary = set()\n",
    "    for token_list in tokens:\n",
    "        vocabulary.update(token_list)\n",
    "    vocabulary = sorted(vocabulary)\n",
    "    # Compute term frequency\n",
    "    tf_matrix = np.zeros((len(documents), len(vocabulary)))\n",
    "    for i, token_list in enumerate(tokens):\n",
    "        for token in token_list:\n",
    "            if token in vocabulary:\n",
    "                tf_matrix[i, vocabulary.index(token)] += 1\n",
    "    # Compute inverse document frequency\n",
    "    df = np.sum(tf_matrix > 0, axis=0)\n",
    "    idf = np.log(tf_matrix.shape[0] / df)\n",
    "    # Compute TF-IDF matrix\n",
    "    tfidf_matrix = tf_matrix * idf\n",
    "    # Normalize TF-IDF matrix\n",
    "    tfidf_norm = np.linalg.norm(tfidf_matrix, axis=1, keepdims=True)\n",
    "    tfidf_matrix /= tfidf_norm\n",
    "    return tfidf_matrix\n",
    "\n",
    "start_time = time.time()\n",
    "tfidf_matrix_np = calculate_tfidf_numpy([preprocessed_text])\n",
    "similarity_np = np.dot(tfidf_matrix_np, tfidf_matrix_np.T)\n",
    "print(\"Optimized Numpy Cosine Similarity Calculation:\")\n",
    "print(\"Time taken:\", time.time() - start_time)\n",
    "\n",
    "# Modified cosine similarity function with memoization\n",
    "@lru_cache(maxsize=None)\n",
    "def cosine_similarity_memoized(documents):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "    similarity_matrix = np.dot(tfidf_matrix, tfidf_matrix.T).toarray()\n",
    "    return similarity_matrix\n",
    "\n",
    "# Test and measure the performance of the memoized version\n",
    "start_time = time.time()\n",
    "similarity_memoized = cosine_similarity_memoized(tuple([preprocessed_text]))  # Convert list to tuple for hashability\n",
    "print(\"Memoized Cosine Similarity Calculation:\")\n",
    "print(\"Time taken:\", time.time() - start_time)\n",
    "\n",
    "# Preprocess text outside parallelized function\n",
    "preprocessed_text = preprocess_text(text)\n",
    "\n",
    "# Function for cosine similarity calculation without vectorization\n",
    "from scipy.sparse import vstack\n",
    "\n",
    "def calculate_tfidf_matrix(text):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform([text])\n",
    "    tfidf_matrix_dense = tfidf_matrix.toarray()  # Convert sparse matrix to dense array\n",
    "    return tfidf_matrix_dense.reshape(1, -1)  # Reshape to ensure 2-D array\n",
    "\n",
    "# Joblib parallelization for TF-IDF matrix calculation\n",
    "start_time = time.time()\n",
    "tfidf_matrices = Parallel(n_jobs=-1)(delayed(calculate_tfidf_matrix)(preprocessed_text) for _ in range(10))\n",
    "\n",
    "# Combine TF-IDF matrices\n",
    "combined_tfidf_matrix = np.concatenate(tfidf_matrices)\n",
    "\n",
    "# Reshape the combined matrix to 2-D\n",
    "combined_tfidf_matrix = combined_tfidf_matrix.reshape(len(tfidf_matrices), -1)\n",
    "\n",
    "# Compute cosine similarity matrix\n",
    "similarity_matrix_joblib = np.dot(combined_tfidf_matrix, combined_tfidf_matrix.T)\n",
    "print(\"Joblib Optimized Cosine Similarity Calculation:\")\n",
    "print(\"Time taken:\", time.time() - start_time)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af1a6c2-1c77-411d-b804-13218d0a4770",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c8b15d-611d-4a1d-9c88-845db24be33a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
